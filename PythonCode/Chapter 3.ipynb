{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter3.DataTransformation import LowPassFilter\n",
    "from Chapter3.DataTransformation import PrincipalComponentAnalysis\n",
    "from Chapter3.ImputationMissingValues import ImputationMissingValues\n",
    "from Chapter3.KalmanFilters import KalmanFilters\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = './intermediate_datafiles/'\n",
    "dataset = pd.read_csv(dataset_path + 'chapter3_result_outliers.csv', index_col=0)\n",
    "dataset.index = dataset.index.to_datetime()\n",
    "DataViz = VisualizeDataset()\n",
    "# Computer the number of milliseconds covered by an instane based on the first two rows\n",
    "milliseconds_per_instance = (dataset.index[1] - dataset.index[0]).microseconds/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "\n",
    "\n",
    "#All features execpt hart rate\n",
    "#train_data_x =data_without_null[['acc_phone_x','acc_phone_y','acc_phone_z']]\n",
    "#Only hart rate\n",
    "train_data_x = dataset[['acc_phone_x','acc_phone_y','acc_phone_z']][~dataset.hr_watch_rate.isnull()]\n",
    "\n",
    "train_data_y = dataset['hr_watch_rate'][~dataset.hr_watch_rate.isnull()]\n",
    "#train_data_x.drop('hr_watch_rate', axis=1, inplace=True)\n",
    "\n",
    "#training\n",
    "linreg.fit(train_data_x,train_data_y)\n",
    "\n",
    "#predict\n",
    "test_data = dataset[['acc_phone_x','acc_phone_y','acc_phone_z']][dataset.hr_watch_rate.isnull()]\n",
    "hr_predicted = pd.DataFrame(linreg.predict(test_data))\n",
    "\n",
    "#replace missing values\n",
    "dataset.hr_watch_rate[dataset.hr_watch_rate.isnull()] = hr_predicted.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DataViz.plot_imputed_values(dataset, [ 'interpolation'], 'hr_watch_rate', dataset['hr_watch_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.hr_watch_rate.iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the result from the previous chapter, and make sure the index is of the type datetime.\n",
    "dataset_path = './intermediate_datafiles/'\n",
    "dataset = pd.read_csv(dataset_path + 'chapter3_result_outliers.csv', index_col=0)\n",
    "dataset.index = dataset.index.to_datetime()\n",
    "# Computer the number of milliseconds covered by an instane based on the first two rows\n",
    "milliseconds_per_instance = (dataset.index[1] - dataset.index[0]).microseconds/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KalFilter = KalmanFilters()\n",
    "kalman_dataset = KalFilter.apply_kalman_filter(dataset, 'hr_watch_rate')\n",
    "DataViz.plot_imputed_values(kalman_dataset, ['original', 'kalman'], 'hr_watch_rate', kalman_dataset['hr_watch_rate_kalman'])\n",
    "DataViz.plot_dataset(kalman_dataset, ['hr_watch_rate', 'hr_watch_rate_kalman'], ['exact','exact'], ['line', 'line'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6.2 Coding Question 2\n",
    " To generate Figs. 3.8 and 3.9 we have used the parameter settings described in\n",
    "Sect. 3.5.1. Vary the constant c (smaller and larger values) of the Chauvenet’s\n",
    "criterion and study the dependency of the number of detected outliers on c.\n",
    "Repeat this for the other three methods presented for outlier detection. Use the\n",
    "source code from book’s website, that generated the figures, as a starting point\n",
    "for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter3.OutlierDetection import DistributionBasedOutlierDetection\n",
    "from Chapter3.OutlierDetection import DistanceBasedOutlierDetection\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let is create our visualization class again.\n",
    "DataViz = VisualizeDataset()\n",
    "\n",
    "# Read the result from the previous chapter, and make sture the index is of the type datetime.\n",
    "dataset_path = './intermediate_datafiles/'\n",
    "try:\n",
    "    dataset = pd.read_csv(dataset_path + 'chapter2_result.csv', index_col=0)\n",
    "except IOError as e:\n",
    "    print('File not found, try to run previous crowdsignals scripts first!')\n",
    "    raise e\n",
    "\n",
    "dataset.index = dataset.index.to_datetime()\n",
    "\n",
    "# Compute the number of milliseconds covered by an instance based on the first two rows\n",
    "milliseconds_per_instance = (dataset.index[1] - dataset.index[0]).microseconds/1000\n",
    "\n",
    "# Step 1: Let us see whether we have some outliers we would prefer to remove.\n",
    "\n",
    "# Determine the columns we want to experiment on.\n",
    "outlier_columns = ['acc_phone_x', 'light_phone_lux']\n",
    "\n",
    "# Create the outlier classes.\n",
    "OutlierDistr = DistributionBasedOutlierDetection()\n",
    "OutlierDist = DistanceBasedOutlierDetection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Determine the columns we want to experiment on.\n",
    "outlier_columns = [ 'light_phone_lux']\n",
    "\n",
    "# Create the outlier classes.\n",
    "OutlierDistr = DistributionBasedOutlierDetection()\n",
    "OutlierDist = DistanceBasedOutlierDetection()\n",
    "\n",
    "for col in outlier_columns:\n",
    "    #dataset = OutlierDistr.chauvenet(dataset, col,100)\n",
    "    #DataViz.plot_binary_outliers(dataset, col, col + '_outlier')\n",
    "#     dataset = OutlierDistr.mixture_model(dataset, col,100,10)\n",
    "#     DataViz.plot_dataset(dataset, [col, col + '_mixture'], ['exact','exact'], ['line', 'points'])\n",
    "    # This requires:\n",
    "    # n_data_points * n_data_points * point_size =\n",
    "    # 31839 * 31839 * 64 bits = ~8GB available memory\n",
    "#     try:\n",
    "#         dataset = OutlierDist.simple_distance_based(dataset, [col], 'euclidean', 0.01, 0.99)\n",
    "#         DataViz.plot_binary_outliers(dataset, col, 'simple_dist_outlier')\n",
    "#     except MemoryError as e:\n",
    "#         print('Not enough memory available for simple distance-based outlier detection...')\n",
    "#         print('Skipping.')\n",
    "    \n",
    "    try:\n",
    "        dataset = OutlierDist.local_outlier_factor(dataset, [col], 'euclidean', 50)\n",
    "        DataViz.plot_dataset(dataset, [col, 'lof'], ['exact','exact'], ['line', 'points'])\n",
    "    except MemoryError as e:\n",
    "        print('Not enough memory available for lof...')\n",
    "        print('Skipping.')\n",
    "#     # Remove all the stuff from the dataset again.\n",
    "    cols_to_remove = [col + '_outlier', col + '_mixture', 'simple_dist_outlier', 'lof']\n",
    "    for to_remove in cols_to_remove:\n",
    "        if to_remove in dataset:\n",
    "            del dataset[to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
