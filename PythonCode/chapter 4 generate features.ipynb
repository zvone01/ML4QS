{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gossa\\Anaconda2\\envs\\ML_1\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Gossa\\Anaconda2\\envs\\ML_1\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 4                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter4.TemporalAbstraction import NumericalAbstraction\n",
    "from Chapter4.TemporalAbstraction import CategoricalAbstraction\n",
    "from Chapter4.FrequencyAbstraction import FourierTransformation\n",
    "from Chapter4.TextAbstraction import TextAbstraction\n",
    "import copy\n",
    "import pandas as pd\n",
    "from Chapter3.DataTransformation import PrincipalComponentAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gossa\\Anaconda2\\envs\\ML_1\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: to_datetime is deprecated. Use pd.to_datetime(...)\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Let us create our visualization class again.\n",
    "DataViz = VisualizeDataset()\n",
    "\n",
    "# Read the result from the previous chapter, and make sure the index is of the type datetime.\n",
    "dataset_path ='./intermediate_datafiles/ourdata/'\n",
    "try:\n",
    "    dataset = pd.read_csv(dataset_path + 'chapter3_result_final.csv', index_col=0)\n",
    "except IOError as e:\n",
    "    print('File not found, try to run previous crowdsignals scripts first!')\n",
    "    raise e\n",
    "\n",
    "dataset.index = dataset.index.to_datetime()\n",
    "\n",
    "# Compute the number of milliseconds covered by an instane based on the first two rows\n",
    "milliseconds_per_instance = (dataset.index[1] - dataset.index[0]).microseconds/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the PC's for all but our target columns (the labels and the heart rate)\n",
    "# We simplify by ignoring both, we could also ignore one first, and apply a PC to the remainder.\n",
    "\n",
    "PCA = PrincipalComponentAnalysis()\n",
    "selected_predictor_cols = [c for c in dataset.columns if (not ('label' in c)) and (not (c == 'hr_watch_rate'))]\n",
    "pc_values = PCA.determine_pc_explained_variance(dataset, selected_predictor_cols)\n",
    "\n",
    "# We select 7 as the best number of PC's as this explains most of the variance\n",
    "\n",
    "n_pcs = 7\n",
    "\n",
    "dataset = PCA.apply_pca(copy.deepcopy(dataset), selected_predictor_cols, n_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelOnTable\n",
      "labelWalking\n",
      "labelRunning\n",
      "Number of patterns of size 1 is 3\n",
      "labelOnTable(b)labelOnTable\n",
      "labelOnTable(b)labelWalking\n",
      "labelWalking(b)labelWalking\n",
      "labelWalking(b)labelRunning\n",
      "labelRunning(b)labelRunning\n",
      "Number of patterns of size 2 is 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chapter 4: Identifying aggregate attributes.\n",
    "\n",
    "# First we focus on the time domain.\n",
    "\n",
    "# Set the window sizes to the number of instances representing 5 seconds, 30 seconds and 5 minutes\n",
    "window_sizes = [int(float(10000)/milliseconds_per_instance), int(float(50000)/milliseconds_per_instance)]\n",
    "#, int(float(5*60000)/milliseconds_per_instance)\n",
    "NumAbs = NumericalAbstraction()\n",
    "dataset_copy = copy.deepcopy(dataset)\n",
    "for ws in window_sizes:\n",
    "    dataset_copy = NumAbs.abstract_numerical(dataset_copy, ['acc_phone_x'], ws, 'mean')\n",
    "    dataset_copy = NumAbs.abstract_numerical(dataset_copy, ['acc_phone_x'], ws, 'std')\n",
    "\n",
    "\n",
    "ws = int(float(50000)/milliseconds_per_instance)\n",
    "selected_predictor_cols = [c for c in dataset.columns if not 'label' in c]\n",
    "dataset = NumAbs.abstract_numerical(dataset, selected_predictor_cols, ws, 'mean')\n",
    "dataset = NumAbs.abstract_numerical(dataset, selected_predictor_cols, ws, 'std')\n",
    "\n",
    "\n",
    "CatAbs = CategoricalAbstraction()\n",
    "dataset = CatAbs.abstract_categorical(dataset, ['label'], ['like'], 0.03, int(float(50000)/milliseconds_per_instance), 2)\n",
    "\n",
    "# Now we move to the frequency domain, with the same window size.\n",
    "\n",
    "FreqAbs = FourierTransformation()\n",
    "fs = float(10000)/milliseconds_per_instance\n",
    "\n",
    "periodic_predictor_cols = [\"acc_phone_x\",\"acc_phone_y\",\"acc_phone_z\",\"gyr_phone_x\",\"gyr_phone_y\",\"gyr_phone_z\",\"step_counter_steps\",\"light_phone_lux\",\"mag_phone_x\",\"mag_phone_y\",\"mag_phone_z\"]\n",
    "\n",
    "data_table = FreqAbs.abstract_frequency(copy.deepcopy(dataset), ['acc_phone_x'], int(float(50000)/milliseconds_per_instance), fs)\n",
    "\n",
    "# Spectral analysis.\n",
    "\n",
    "\n",
    "dataset = FreqAbs.abstract_frequency(dataset, periodic_predictor_cols, int(float(50000)/milliseconds_per_instance), fs)\n",
    "\n",
    "# Now we only take a certain percentage of overlap in the windows, otherwise our training examples will be too much alike.\n",
    "\n",
    "# The percentage of overlap we allow\n",
    "window_overlap = 0.9\n",
    "skip_points = int((1-window_overlap) * ws)\n",
    "dataset = dataset.iloc[::skip_points,:]\n",
    "\n",
    "\n",
    "dataset.to_csv(dataset_path + 'chapter5_result.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milliseconds_per_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
