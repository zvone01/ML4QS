{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 8                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter7.PrepareDatasetForLearning import PrepareDatasetForLearning\n",
    "from Chapter7.Evaluation import RegressionEvaluation\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalClassificationAlgorithms\n",
    "from Chapter8.LearningAlgorithmsTemporal import TemporalRegressionAlgorithms\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "from util import util\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Of course we repeat some stuff from Chapter 3, namely to load the dataset\n",
    "\n",
    "DataViz = VisualizeDataset()\n",
    "\n",
    "# Read the result from the previous chapter, and make sure the index is of the type datetime.\n",
    "dataset_path = './intermediate_datafiles/ourdata/'\n",
    "\n",
    "try:\n",
    "    dataset = pd.read_csv(dataset_path + 'chapter5_result.csv', index_col=0)\n",
    "except IOError as e:\n",
    "    print('File not found, try to run previous crowdsignals scripts first!')\n",
    "    raise e\n",
    "\n",
    "dataset.index = dataset.index.to_datetime()\n",
    "\n",
    "# Let us consider our second task, namely the prediction of the heart rate. We consider this as a temporal task.\n",
    "\n",
    "prepare = PrepareDatasetForLearning()\n",
    "\n",
    "train_X, test_X, train_y, test_y = prepare.split_single_dataset_regression_by_time(dataset, 'hr_watch_rate', '2016-02-08 18:29:56',\n",
    "#                                                                                   '2016-02-08 18:29:58','2016-02-08 18:29:59')\n",
    "                                                                                   '2016-02-08 19:34:07', '2016-02-08 20:07:50')\n",
    "\n",
    "print 'Training set length is: ', len(train_X.index)\n",
    "print 'Test set length is: ', len(test_X.index)\n",
    "\n",
    "# Select subsets of the features that we will consider:\n",
    "\n",
    "print 'Training set length is: ', len(train_X.index)\n",
    "print 'Test set length is: ', len(test_X.index)\n",
    "\n",
    "# Select subsets of the features that we will consider:\n",
    "\n",
    "basic_features = ['acc_phone_x','acc_phone_y','acc_phone_z','acc_watch_x','acc_watch_y','acc_watch_z','gyr_phone_x','gyr_phone_y','gyr_phone_z','gyr_watch_x','gyr_watch_y','gyr_watch_z',\n",
    "                  'labelOnTable','labelSitting','labelWashingHands','labelWalking','labelStanding','labelDriving','labelEating','labelRunning',\n",
    "                  'light_phone_lux','mag_phone_x','mag_phone_y','mag_phone_z','mag_watch_x','mag_watch_y','mag_watch_z','press_phone_pressure']\n",
    "pca_features = ['pca_1','pca_2','pca_3','pca_4','pca_5','pca_6','pca_7']\n",
    "time_features = [name for name in dataset.columns if ('temp_' in name and not 'hr_watch' in name)]\n",
    "freq_features = [name for name in dataset.columns if (('_freq' in name) or ('_pse' in name))]\n",
    "print '#basic features: ', len(basic_features)\n",
    "print '#PCA features: ', len(pca_features)\n",
    "print '#time features: ', len(time_features)\n",
    "print '#frequency features: ', len(freq_features)\n",
    "cluster_features = ['cluster']\n",
    "print '#cluster features: ', len(cluster_features)\n",
    "features_after_chapter_3 = list(set().union(basic_features, pca_features))\n",
    "features_after_chapter_4 = list(set().union(basic_features, pca_features, time_features, freq_features))\n",
    "features_after_chapter_5 = list(set().union(basic_features, pca_features, time_features, freq_features, cluster_features))\n",
    "\n",
    "selected_features = ['temp_pattern_labelOnTable','labelOnTable', 'temp_pattern_labelOnTable(b)labelOnTable', 'cluster',\n",
    "                     'pca_1_temp_mean_ws_120','pca_2_temp_mean_ws_120','pca_2','acc_watch_y_temp_mean_ws_120','gyr_watch_y_pse',\n",
    "                     'gyr_watch_x_pse']\n",
    "possible_feature_sets = [basic_features, features_after_chapter_3, features_after_chapter_4, features_after_chapter_5, selected_features]\n",
    "feature_names = ['initial set', 'Chapter 3', 'Chapter 4', 'Chapter 5', 'Selected features']\n",
    "\n",
    "# Let us first study whether the time series is stationary and what the autocorrelations are.\n",
    "\n",
    "dftest = adfuller(dataset['hr_watch_rate'], autolag='AIC')\n",
    "print dftest\n",
    "\n",
    "autocorrelation_plot(dataset['hr_watch_rate'])\n",
    "plot.show()\n",
    "\n",
    "# Now let us focus on the learning part.\n",
    "\n",
    "learner = TemporalRegressionAlgorithms()\n",
    "eval = RegressionEvaluation()\n",
    "\n",
    "# We repeat the experiment a number of times to get a bit more robust data as the initialization of the NN is random.\n",
    "\n",
    "repeats = 5\n",
    "\n",
    "# we set a washout time to give the NN's the time to stabilize. We do not compute the error during the washout time.\n",
    "\n",
    "washout_time = 10\n",
    "\n",
    "scores_over_all_algs = []\n",
    "\n",
    "for i in range(0, len(possible_feature_sets)):\n",
    "\n",
    "    selected_train_X = train_X[possible_feature_sets[i]]\n",
    "    selected_test_X = test_X[possible_feature_sets[i]]\n",
    "\n",
    "    # First we run our non deterministic classifiers a number of times to average their score.\n",
    "\n",
    "    performance_tr_res = 0\n",
    "    performance_tr_res_std = 0\n",
    "    performance_te_res = 0\n",
    "    performance_te_res_std = 0\n",
    "    performance_tr_rnn = 0\n",
    "    performance_tr_rnn_std = 0\n",
    "    performance_te_rnn = 0\n",
    "    performance_te_rnn_std = 0\n",
    "\n",
    "    for repeat in range(0, repeats):\n",
    "        print '----', repeat\n",
    "        regr_train_y, regr_test_y = learner.reservoir_computing(selected_train_X, train_y, selected_test_X, test_y, gridsearch=True, per_time_step=False)\n",
    "\n",
    "        mean_tr, std_tr = eval.mean_squared_error_with_std(train_y.ix[washout_time:,], regr_train_y.ix[washout_time:,])\n",
    "        mean_te, std_te = eval.mean_squared_error_with_std(test_y.ix[washout_time:,], regr_test_y.ix[washout_time:,])\n",
    "\n",
    "        performance_tr_res += mean_tr\n",
    "        performance_tr_res_std += std_tr\n",
    "        performance_te_res += mean_te\n",
    "        performance_te_res_std += std_te\n",
    "\n",
    "        regr_train_y, regr_test_y = learner.recurrent_neural_network(selected_train_X, train_y, selected_test_X, test_y, gridsearch=True)\n",
    "\n",
    "        mean_tr, std_tr = eval.mean_squared_error_with_std(train_y.ix[washout_time:,], regr_train_y.ix[washout_time:,])\n",
    "        mean_te, std_te = eval.mean_squared_error_with_std(test_y.ix[washout_time:,], regr_test_y.ix[washout_time:,])\n",
    "\n",
    "        performance_tr_rnn += mean_tr\n",
    "        performance_tr_rnn_std += std_tr\n",
    "        performance_te_rnn += mean_te\n",
    "        performance_te_rnn_std += std_te\n",
    "\n",
    "\n",
    "    # We only apply the time series in case of the basis features.\n",
    "    if (feature_names[i] == 'initial set'):\n",
    "        regr_train_y, regr_test_y = learner.time_series(selected_train_X, train_y, selected_test_X, test_y, gridsearch=True)\n",
    "\n",
    "        mean_tr, std_tr = eval.mean_squared_error_with_std(train_y.ix[washout_time:,], regr_train_y.ix[washout_time:,])\n",
    "        mean_te, std_te = eval.mean_squared_error_with_std(test_y.ix[washout_time:,], regr_test_y.ix[washout_time:,])\n",
    "\n",
    "        overall_performance_tr_ts = mean_tr\n",
    "        overall_performance_tr_ts_std = std_tr\n",
    "        overall_performance_te_ts = mean_te\n",
    "        overall_performance_te_ts_std = std_te\n",
    "    else:\n",
    "        overall_performance_tr_ts = 0\n",
    "        overall_performance_tr_ts_std = 0\n",
    "        overall_performance_te_ts = 0\n",
    "        overall_performance_te_ts_std = 0\n",
    "\n",
    "    overall_performance_tr_res = performance_tr_res/repeats\n",
    "    overall_performance_tr_res_std = performance_tr_res_std/repeats\n",
    "    overall_performance_te_res = performance_te_res/repeats\n",
    "    overall_performance_te_res_std = performance_te_res_std/repeats\n",
    "    overall_performance_tr_rnn = performance_tr_rnn/repeats\n",
    "    overall_performance_tr_rnn_std = performance_tr_rnn_std/repeats\n",
    "    overall_performance_te_rnn = performance_te_rnn/repeats\n",
    "    overall_performance_te_rnn_std = performance_te_rnn_std/repeats\n",
    "\n",
    "    scores_with_sd = [(overall_performance_tr_res, overall_performance_tr_res_std, overall_performance_te_res, overall_performance_te_res_std),\n",
    "                      (overall_performance_tr_rnn, overall_performance_tr_rnn_std, overall_performance_te_rnn, overall_performance_te_rnn_std),\n",
    "                      (overall_performance_tr_ts, overall_performance_tr_ts_std, overall_performance_te_ts, overall_performance_te_ts_std)]\n",
    "    print scores_with_sd\n",
    "    util.print_table_row_performances_regression(feature_names[i], len(selected_train_X.index), len(selected_test_X.index), scores_with_sd)\n",
    "    scores_over_all_algs.append(scores_with_sd)\n",
    "\n",
    "DataViz.plot_performances_regression(['Reservoir', 'RNN', 'Time series'], feature_names, scores_over_all_algs)\n",
    "\n",
    "regr_train_y, regr_test_y = learner.reservoir_computing(train_X[features_after_chapter_5], train_y, test_X[features_after_chapter_5], test_y, gridsearch=True)\n",
    "DataViz.plot_numerical_prediction_versus_real(train_X.index, train_y, regr_train_y['hr_watch_rate'], test_X.index, test_y, regr_test_y['hr_watch_rate'], 'heart rate')\n",
    "regr_train_y, regr_test_y = learner.recurrent_neural_network(train_X[basic_features], train_y, test_X[basic_features], test_y, gridsearch=True)\n",
    "DataViz.plot_numerical_prediction_versus_real(train_X.index, train_y, regr_train_y['hr_watch_rate'], test_X.index, test_y, regr_test_y['hr_watch_rate'], 'heart rate')\n",
    "regr_train_y, regr_test_y = learner.time_series(train_X[basic_features], train_y, test_X[features_after_chapter_5], test_y, gridsearch=True)\n",
    "DataViz.plot_numerical_prediction_versus_real(train_X.index, train_y, regr_train_y['hr_watch_rate'], test_X.index, test_y, regr_test_y['hr_watch_rate'], 'heart rate')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
